---
title: "MATH 392 Problem Set 3"
author: "EJ Arce"
date: "7 February 2018"
output: 
  pdf_document:
    fig_height: 3
    fig_width: 6
---


```{r, include=FALSE}
require(tidyverse) # load packages
require(resampledata)
set.seed(30) # set seed for reproducibility
```

## 4.8

$$
n = 20, \mu = 6, \sigma^{2} = 10
$$

$$
P(\bar{X} \leq 4.6) =
P(\frac{\bar{X}-\mu}{\sigma / \sqrt{n}} \leq
\frac{4.6 -\mu}{\sigma / \sqrt{n}})
$$

$$
P(\bar{X} \leq 4.6) =
P(\frac{\bar{X}-6}{\sqrt{10} / \sqrt{20}} \leq
\frac{4.6 -6}{\sqrt{10}  / \sqrt{20}})
$$

```{r}
# Calculate Z score of interested statistic
z.obs <- (4.6-6)/(sqrt(10)/sqrt(20))
z.obs
```

$$
P(\bar{X} \leq 4.6) = P(Z \leq -1.98)
$$

```{r}
# Calculate probability using the cdf of N(0,1)
pnorm(z.obs, 0, 1)
```

$$
P(\bar{X} \leq 4.6) = .02385
$$

## 4.9

$$
f_{X}(x) = \frac{3}{16}(x-4)^{2} for 2 \leq 6
$$

Find E[X]:

$$
E[X] = \int_{2}^{6} x \frac{3}{16}(x-4)^{2} dx
$$

$$
E[X] = \int_{2}^{6} \frac{3}{16}x(x^{2}-8x+16) dx
$$

$$
E[X] = \int_{2}^{6} \frac{3}{16}x^{3}- \frac{3}{2}x^{2}+3x dx
$$

$$
E[X] = \frac{3}{64}x^{4} - \frac{1}{2}x^{3} + \frac{3}{2}x^{2} \vert_{2}^{6}
$$

$$
E[X] = 4
$$

Find V[X]:

$$
V[X] = E[X^{2}] - E[X]^{2}
$$

We already calculated that $E[X]=4$, so $E[X]^{2}$ = 16. Now solve for $E[X^2]$:

$$
E[X^{2}] = \int_{2}^{6} x^{2} f(x) dx
$$

$$
E[X^{2}] = \int_{2}^{6} x^{2} \frac{3}{16}(x-4)^{2} dx
$$

$$
E[X^{2}] = \int_{2}^{6} \frac{3}{16}x^{4}- \frac{3}{2}x^{3}+3x^{2} dx
$$

$$
E[X^{2}] = \frac{3}{80}x^{5}- \frac{3}{8}x^{4}+x^{3} \vert_{2}^{6}
$$

```{r}
# Calculate
e.xsq <- (3*(6^5)/80 - 3*(6^4)/8 +6^3) -
  (3*(2^5)/80 - 3*(2^4)/8 +(2^3))
e.xsq
sq.ex <- 4^2
var.x <- e.xsq-sq.ex
sd.x <- sqrt(var.x)
sd.x
```

Thus $V[X] = 2.4$, so SD[X] = $\sqrt{2.4}$ = 1.549. Now, $n = 244, \mu = 4, \sigma^{2} = 2.4$.

$$
P(\bar{X} \geq 4.2) =
P(\frac{\bar{X}-\mu}{\sigma / \sqrt{n}} \geq
\frac{4.2 -\mu}{\sigma / \sqrt{n}})
$$

$$
P(\bar{X} \geq 4.2) =
P(\frac{\bar{X}-4}{\sqrt{2.4} / \sqrt{244}} \geq
\frac{4.2 -4}{\sqrt{2.4} / \sqrt{244}})
$$

$$
P(\bar{X} \geq 4.2) =
P(Z \geq \frac{4.2 -4}{\sqrt{2.4} / \sqrt{244}})
$$

```{r}
# Calculate Z score of interested statistic
z.obs <- (4.2-4)/(sqrt(2.4)/sqrt(244))
z.obs
```

```{r}
# Calculate probability using cdf from N(0,1)
1 - pnorm(z.obs,0,1)
```


## 4.12

### a

Let X be a random sample of size 30 from the exponential distribution with rate $\lambda = .1$. The expected value of the sample mean is the same as the expected value of the population, by linearity of expectation. Thus, E[X] = $\frac{1}{\lambda}$ = 10

### b

```{r}
# Run simulation
nsim <- 1000
n <- 30
rate <- 1/10
means <- rep(NA,nsim)
for(i in 1:nsim){
  sample <- rexp(n,rate)
  means[i] <- mean(sample)
}
sum(means >= 12)/nsim
```

### c

Since 12.1% of the samples had means of 12 or greater, this observation is not that unusual.

## 4.13

### a

Since X ~ $N(20,8^{2})$ and Y $~ N(16,7^{2})$ are independent variables, and W = $\bar{X} + \bar{Y}$, then W ~ $N(36,\frac{8^{2}}{10} + \frac{7^{2}}{15})$

### b

```{r}
nsim <- 10000
w <- rep(NA,nsim)
for(i in 1:nsim){
  x <- rnorm(10,20,8)
  y <- rnorm(15,16,7)
  w[i] <- mean(x) + mean(y)
}
# Compute mean and standard error
mean(w)
sd(w)
# Plot sampling distribution
w <- data.frame(w)
ggplot(w,aes(x=w)) +
  geom_density() +
  geom_vline(xintercept=40)
```

### c

```{r}
(sum(w<40) + 1) /( nsim + 1)
```


## 4.18

### a

```{r}
# Simulate sampling distribution
nsim <- 10000
n <- 30
rate <- 1/3
x.bars <- rep(NA,nsim)
for(i in 1:nsim){
  sample <- rexp(n,rate)
  x.bars[i] <- mean(sample)
}
```

### b

```{r}
# Compute and compare simulated mean and standard error with theoretical results
mean.sim <- mean(x.bars)
se.sim <- sd(x.bars)
mean.theory <- 1/rate
se.theory <- (1/rate)/sqrt(n)
mean.sim
mean.theory
se.sim
se.theory
```

### c

```{r}
# Calculate simulated probability
d <- data.frame(x.bars)
(sum(x.bars <= 3.5) + 1) / (nsim + 1)
```

### d

$$
n = 30, X ~ exp(1/3), \mu = 1/3, \sigma^{2} = 9
$$

$$
P(\bar{X} \leq 3.5) = 
P(
\frac{\bar{X} - \mu}{\sqrt{\sigma^{2}}/\sqrt{n}} \leq
\frac{3.5 - \mu}{\sqrt{\sigma^{2}}/\sqrt{n}}
) =
P(
\frac{\bar{X} - 3}{\sqrt{9}/\sqrt{30}} \leq
\frac{3.5 - 3}{\sqrt{9}/\sqrt{30}}
)
$$

```{r}
# Calculate z score we are testing
z.test <- (3.5-3)/(sqrt(9)/sqrt(30))
z.test
```

$$
P(\bar{X} \leq 3.5) =
P(Z \leq .9129)
$$

```{r}
# Calculate approximated probability
pnorm(z.obs,0,1)
```

$P(\bar{X} \leq 3.5) = .8193$. The approximated result is similar to the simulated probability of .8252.

## 4.20

$Let X_{1},...,X_{n} be continuouus and i.i.d. random variables with pdf f and cdf F. Show that the pdf's for X_{min} and X_{max} are

$$
f_{min}(x) = n(1-F(x))^{n-1}f(x)
$$

$$
f_{max}(x) = nF(x)^{n-1}(x)f(x)
$$

First, show the pdf of $X_{max}$:

$$
F_{max}(x) = P(max{X_{1},...,X_{n}} \leq x)
$$

$$
F_{max}(x) = P(X_{1} \leq x,...,X_{n} \leq x)
$$

Because the variables are i.i.d.,

$$
F_{max}(x) = P(X_{1} \leq x)...P(X_{n} \leq x)
$$

$$
F_{max}(x) = F(x)...F(x) = F(x)^n
$$

Now, differentiate to find $f_{max}(x)$:

$$
f_{max}(x) = \frac{\partial}{\partial x} F_{max}(x) = nF(x)^{n-1}f(x)
$$

Now show the pdf$f_{min}(x)$:

$$
F_{min}(x) = P(min{X_{1},...,X_{n}} \leq x)
$$

At least one of the variables $X_{i} \leq x$, so we can use the probability that none of the random variables will be less than x, and subtract that from 1:

$$
F_{min}(x) = (1 - P(X_{1}\leq x))...(1 - P(X_{n}\leq x))
$$

$$
F_{min}(x) = (1 - P(X\leq x))^{n} = (1-F(x))^{n}
$$

Now differentiate to find $f_{min}(x)$:

$$
f_{min}(x) =
\frac{\partial}{\partial x} F_{min}(x) =
n(1-F(x))^{n-1}f(x)
$$

## 4.21

### a

By theorem 4.1, $f_{max}(x) = nF(x)^{n-1}f(x)$. In this case, n = 2, and $f(x) = 2/x^{2} for 1 \leq x \leq 2$. So

$$
f_{max}(x) = 2(2/x^{2})\int_{1}^{x} 2/x^{-2} dx
$$

$$
f_{max}(x) = 2(2/x^{2}) (-2x^{-1} \vert_{1}^{x}) 
$$

$$
f_{max}(x) = 2(2/x^{2}) (2(1-1/x))
$$

$$
f_{max}(x) = \frac{8-8/x}{x^{2}}
$$

### b

Solve for E[X]:

$$
E[X] = \int_{1}^{2}xf_{max}(x) dx
$$

$$
E[X] = \int_{1}^{2}x \frac{8-8/x}{x^{2}} dx
$$

$$
E[X] = \int_{1}^{2}8x^{-1} - 8x^{-2} dx
$$

$$
E[X] = 8lnx + 8x^{-1} \vert_{1}^{2} = (8 ln2 + 4) - (8ln1 + 8)
$$

$$
E[X] = (8 ln2 + 4) - (8ln1 + 8) = 8ln2+4 - (0+8)
$$

$$
E[X] \approx 1.545.
$$


## 5.2

```{r}
dist <- c(1,3,4,6)
nsim <- 10000
means <- rep(NA,nsim)
maxs <- rep(NA,nsim)
for(i in 1:nsim){
  boot <- sample(dist,4,replace=T)
  means[i] <- mean(boot)
  maxs[i] <- max(boot)
}
```


### a

```{r}
(sum(means = 1) + 1)/(nsim + 1)
```


### b

```{r}
(sum(maxs = 6) + 1)/(nsim + 1)
```


### c

```{r}
indicator <- rep(NA,nsim)
for(i in 1:nsim){
  boot <- sample(dist,4,replace=T)
  sum <- sum(boot == 1)
  if(sum == 2){
    indicator[i] <- 1
  }
  else{
    indicator[i] <- 0
  }
}
mean(indicator)
```


## 5.8

### a

```{r}
# Similate sampling distribution
nsim <- 1000
n <- 200
shape <- 5
rate <- 1/4
means <- rep(NA,nsim)
for(i in 1:nsim){
  sample <- rgamma(n,shape,rate)
  means[i] <- mean(sample)
}
df.sample <- data.frame(means)
samp.dist <- ggplot(df.sample, aes(x=means)) +
  geom_density()
samp.dist
```

As expected, the distribution is approximately normal and is centered at 20.

### b

```{r}
sample <- rgamma(n,shape,rate)
df <- data.frame(sample)
ggplot(df,aes(x=sample)) +
  geom_histogram()
mean(sample)
sd(sample)
```

### c

```{r}
means <- rep(NA,nsim)
for(i in 1:nsim){
  boot <- sample(sample,n,replace=T)
  means[i] <- mean(boot)
}
df.boot <- data.frame(means)
boot.dist <- ggplot(df.boot,aes(x=means)) +
  geom_density()
mean <- mean(df.boot$means)
se <- sd(df.boot$means)
boot.dist
mean
se
```


### d

```{r}
ggplot(df.sample, aes(x=means, col ="blue")) +
  geom_density() +
  geom_density(data = df.boot, aes(col="red")) +
  scale_color_manual(labels = c("Approximated sampling distribution",
                                "Bootstrap distribution"),
                     values = c("blue","red"))
```

The two distributions appear similar in skew and shape, but the bootstrap distribution estimates the mean of the population to be higher than the sampling distribution's estimate.

### e

The codes producing the graphs below are exactly the same as the codes for problems a-d, exect n is changed to 50 and 10, respectively.

#### n = 50

```{r,echo=FALSE}
# Similate sampling distribution
nsim <- 1000
n <- 50
shape <- 5
rate <- 1/4
means <- rep(NA,nsim)
for(i in 1:nsim){
  sample <- rgamma(n,shape,rate)
  means[i] <- mean(sample)
}
df.sample <- data.frame(means)
samp.dist <- ggplot(df.sample, aes(x=means)) +
  geom_density()
```

```{r,echo=FALSE}
sample <- rgamma(n,shape,rate)
df <- data.frame(sample)
```

```{r,echo=FALSE}
means <- rep(NA,nsim)
for(i in 1:nsim){
  boot <- sample(sample,n,replace=T)
  means[i] <- mean(boot)
}
df.boot <- data.frame(means)
boot.dist <- ggplot(df.boot,aes(x=means)) +
  geom_density()
```

```{r,echo=FALSE}
ggplot(df.sample, aes(x=means, col ="blue")) +
  geom_density() +
  geom_density(data = df.boot, aes(col="red")) +
  scale_color_manual(labels = c("Approximated sampling distribution",
                                "Bootstrap distribution"),
                     values = c("blue","red"))
```

#### n = 10

```{r,echo=FALSE}
# Similate sampling distribution
nsim <- 1000
n <- 10
shape <- 5
rate <- 1/4
means <- rep(NA,nsim)
for(i in 1:nsim){
  sample <- rgamma(n,shape,rate)
  means[i] <- mean(sample)
}
df.sample <- data.frame(means)
samp.dist <- ggplot(df.sample, aes(x=means)) +
  geom_density()
```

```{r,echo=FALSE}
sample <- rgamma(n,shape,rate)
df <- data.frame(sample)
```

```{r,echo=FALSE}
means <- rep(NA,nsim)
for(i in 1:nsim){
  boot <- sample(sample,n,replace=T)
  means[i] <- mean(boot)
}
df.boot <- data.frame(means)
boot.dist <- ggplot(df.boot,aes(x=means)) +
  geom_density()
```

```{r,echo=FALSE}
ggplot(df.sample, aes(x=means, col ="blue")) +
  geom_density() +
  geom_density(data = df.boot, aes(col="red")) +
  scale_color_manual(labels = c("Approximated sampling distribution",
                                "Bootstrap distribution"),
                     values = c("blue","red"))
```

For both the sampling distribution and the bootstrap distribution, the variance of the mean estimates increases as the sample size, n, decreases. Changing the sample size does not appear to change the mean estimate in any particular way. In each case, the sampling distribution provides a better estimate of the mean than the bootstrap distribution.

## 5.12

### a

```{r}
ggplot(FishMercury,aes(x=Mercury)) +
  geom_histogram(binwidth=.01)
```

The data includes 29 observations with a mercury level of less than .25 parts per million and 1 observation with a mercury level of 1.87 parts per million.

### b

```{r}
data(FishMercury)
nsim <- 10000
n <- 30
means <- rep(NA,nsim)
for(i in 1:nsim){
  boot <- sample(FishMercury$Mercury, n, replace=T)
  means[i] <- mean(boot)
}
se.boot <- sd(means)
se.boot
CI <- quantile(means, probs = c(.025,.975))
CI
```


### c

```{r}
# Remove outlier
FishMercury2 <- FishMercury %>%
  filter(Mercury < 1)

# Repeat simulation
nsim <- 10000
n <- 30
means <- rep(NA,nsim)
for(i in 1:nsim){
  boot <- sample(FishMercury2$Mercury, n, replace=T)
  means[i] <- mean(boot)
}
se.boot <- sd(means)
se.boot
CI <- quantile(means, probs = c(.025,.975))
CI
```


### d

The standard error greatly reduced, now that the outlier can't affect the mean estimate in each bootstrap sample.
