---
title: "MATH 392 Problem Set 4"
author: "EJ Arce"
date: "17 February 2018"
output: 
  pdf_document:
    fig_height: 3
    fig_width: 6
---


```{r, include=FALSE}
require(tidyverse) # load packages
require(resampledata)
set.seed(30) # set seed for reproducibility
```

## 6.2

### Analytical Solution

Let $x_{1},...,x_{n}$ ~ Poisson($\lambda$). Show the maximum likelihood estimate of $\lambda$ is $\hat{\lambda}=\bar{x}$:

$$
L(\lambda|x_{1},...,x_{n}) = P(X_{1} = x_{1},...,X_{n}=x_{n})
$$

$$
L(\lambda|x_{1},...,x_{n}) = P(X = x_{1})...P(X=x_{n})
$$

$$
L(\lambda|x_{1},...,x_{n}) = \prod_{i}^{n}\frac{\lambda^{x_{i}}e^{-\lambda}}{x_{i}!}
$$

$$
L(\lambda|x_{1},...,x_{n}) = \frac{e^{-\lambda n}\lambda^{\sum_{i}^{n}{x_{i}}}}{\prod_{i}^{n}x_{i}!}
$$

Take the natural log of both sides to make the derivation easier:

$$
ln(L(\lambda|x_{1},...,x_{n})) =
ln(e^{-n\lambda}) +
ln(\lambda^{\sum_{i}^{n}{x_{i}}}) -
ln(\prod_{i}^{n}x_{i}!)
$$

$$
ln(L(\lambda|x_{1},...,x_{n})) =
-n\lambda +
ln(\lambda)\sum_{i}^{n}{x_{i}} -
ln(\prod_{i}^{n}x_{i}!)
$$

Derive with respect to $\lambda$ and set equal to 0:

$$
\frac{\partial ln(L(\lambda|x_{1},...,x_{n}))}{\partial \lambda} =
-n +
\frac{\sum_{i}^{n}{x_{i}}}{\lambda} =
0
$$

$$
\frac{\partial ln(L(\lambda|x_{1},...,x_{n}))}{\partial \lambda} =
\lambda = 
\frac{\sum_{i}^{n}{x_{i}}}{n} = \bar{x}
$$

Thus $\lambda_{MLE} = \bar{x}$

### Empirical Solution

```{r}
# Build function
e <- exp(1)
pois <- function(lambda, x){
  n <- length(x)
  sum.x <- sum(x)
  prod.fact <- prod(factorial(x))
  e^(-lambda*n)*(lambda^(sum.x))/(prod.fact)
}

# Create sequence of lambdas for MLE plot
lambdas <- seq(from = 0, to = 20, by = .05)
# Draw random sample of x's
x <- sample(1:10,30,replace=T)
# Apply pois function to find likelihoods
L_pois <- pois(lambdas,x)

# Plot
df <- data.frame(x = lambdas,
                 y = L_pois)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = mean(x), col = "red")
```

A red line above indicates the mean of the random sample. As expected, the highest likelihood for $\lambda$ is at the sample mean.

## 6.8

### Analytical Solution

$$
f(x;\theta) = \frac{\sqrt{2/\pi}x^{2}e^{-x^{2}/2\theta^{2}}}{\theta^{3}}
$$

$$
L(\theta|x_{1},...,x_{n}) =
\prod_{i}^{n}f(x_{i};\theta)
$$

$$
L(\theta|x_{1},...,x_{n}) =
\prod_{i}^{n}
\frac{\sqrt{2/\pi}x_{i}^{2}
e^{-x_{i}^{2}/2\theta^{2}}}
{\theta^{3}}
$$

$$
L(\theta|x_{1},...,x_{n}) =
(\frac{\sqrt{2/\pi}}
{\theta^3})^n
\prod_{i}^{n}
x_{i}^{2}
e^{-x_{i}^{2}/2\theta^{2}}
$$

$$
ln(L(\theta|x_{1},...,x_{n})) =
ln(\frac{\sqrt{2/\pi}}
{\theta^3})^n +
\sum_{i}^{n}
ln(x_{i}^{2}) +
\sum_{i}^{n}
ln(e^{-x_{i}^{2}/2\theta^{2}})
$$

$$
ln(L(\theta|x_{1},...,x_{n})) =
ln\sqrt{2/\pi} -
ln \theta^3 +
\sum_{i}^{n}
ln(x_{i}^{2}) +
\sum_{i}^{n}
-x_{i}^{2}/2\theta^{2}
$$

Derive with respect to $\theta$ and set to 0:

$$
\frac{\partial(L(\theta|x_{1},...,x_{n}))}{\partial\theta} =
-\frac{n}{\theta^3}3\theta^{2} + \sum_i^n{x_i^{2}/\theta^{3}} = 0
$$

$$
\frac{3n}{\theta} = \frac{\sum_i^n x_{i}^2}{\theta^3}
$$

$$
\theta = \frac{
\bar{x}
\sqrt{n}
}
{\sqrt{3}}
$$

Thus $\theta_{MLE}  = \frac{\bar{x}\sqrt{n}}{\sqrt{3}}$.

### Empirical Solution

```{r}
# Build function
f <- function(theta, x){
  e <- exp(1)
  n <- length(x)
  sec.term <- (x^2)*e^((-x^2)/(2*(theta)^2))
  prod.x <- prod(sec.term)
  ((sqrt(2/pi)/theta)^n)*prod.x
}

# Create sequence of thetas for MLE plot
thetas <- seq(from = .1, to = 30, by = .1)
# Reset seed and draw random sample of x's
set.seed(23)
x <- sample(1:10,30,replace=T)
# Apply function to find likelihoods
L_theta <- f(thetas,x)
# Find x-intercept from analytical solution
n <- length(x)
analytical.sol <- mean(x)*sqrt(n)/sqrt(3)
# Plot
df <- data.frame(x = thetas,
                 y = L_theta)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = analytical.sol, col = "red")
```

## 6.11

### Analytical Solution

As before,

$$
L(\lambda) = \prod_i^n f(x_{i}) \prod_j^m f(y_{j})
$$

$$
L(\lambda) =
\prod_i^n \lambda e^{-\lambda x_i}
\prod_j^m \lambda e^{-\lambda y_j}
$$

$$
L(\lambda) =
\lambda^{n+m}\lambda
e^{-\lambda(\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j})}
$$

$$
ln(L(\lambda)) =
(n+m)ln(\lambda) -
\lambda(\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j})
$$

$$
\frac
{\partial ln(L(\lambda))}
{\partial \lambda}=
(n+m)/\lambda -
(\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j}) = 0
$$

$$
\lambda =
\frac{n+m}
{\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j}}
$$

Thus, $\lambda_{MLE} = \frac{n+m}{\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j}}$.

### Empirical Solution

```{r}
# Build function
e <- exp(1)
pois <- function(lambda,x,y){
  n <- length(x)
  m <- length(y)
  p.x <- (lambda^n)*prod((e^(-lambda*x)))
  p.y <- (lambda^m)*prod((e^(-lambda*y)))
  p.x*p.y
}

# Create sequence of lambdas for MLE plot
lambdas <- seq(from = 0, to = 20, by = .05)
# Draw random sample of x's
set.seed(11)
x <- sample(1:10,30,replace=T)
set.seed(35)
y <- sample(1:10,40,replace=T)
# Apply pois function to find likelihoods
L_pois <- pois(lambdas,x,y)
# Calculate analytical solution
obs <- (length(x)+length(y))/(sum(x) + sum(y))
# Plot
df <- data.frame(x = lambdas,
                 y = L_pois)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = obs, col = "red")
```


## 6.13

### a

$$
L(\alpha|X;\beta) =
\alpha \beta x^{\beta - 1} e^{-\alpha x^{\beta}}
$$

$$
ln(L(\alpha|X;\beta)) =
ln\alpha + ln\beta +(\beta - 1)lnx + ln(e^{-\alpha x^{\beta}})
$$

$$
ln(L(\alpha|X;\beta)) =
ln\alpha + ln\beta +(\beta - 1)lnx -\alpha x^{\beta}
$$

$$
\frac{\partial{ln(L(\alpha|X;\beta))}}{\partial \alpha}=
\frac{1}{\alpha} - x^{\beta} = 0
$$

$$
\alpha = \frac{1}{x^{\beta}}
$$

Thus $\alpha_{MLE} = \frac{1}{x^{\beta}}$.

### b

$$
L(\alpha;\beta|X) =
\alpha \beta x^{\beta - 1} e^{-\alpha x^{\beta}}
$$

Take the natural log of both sides:

$$
ln(L(\alpha;\beta|X)) =
ln \alpha + ln \beta + (\beta - 1)ln x -\alpha x^{\beta}
$$

Differentiate with respect to $\alpha$ and $\beta$ and set to 0. Solve simultaneously for a and b:

$$
\frac{\partial ln(L(\alpha|X))}
{\partial \alpha}=
1/\alpha - x^{\beta} = 0
$$

and

$$
\frac{\partial ln(L(\beta|X))}
{\partial \beta}=
1/\beta - +ln x - ax^{\beta} = 0
$$


## 6.14

Let X = 2,3,5,9,10 ~ Unif[$\alpha,\beta$]. Find $\hat{\alpha}_{MOM}$ and $\hat{\beta}_{MOM}$:

1. Write first and second moments in terms of parameters $\alpha$ and $\beta$

$$
\mu_{1} =
E[X] =
\int_{\alpha}^{\beta} \frac{1}{\beta-\alpha} x dx = 
\frac{1}{2(\beta-\alpha)} x^2 \vert_{\alpha}^{\beta} = 
\frac{\alpha +\beta}{2} = 
5.8 = 
\bar{X}
$$

Thus

$$
\alpha +\beta = 11.6
$$

Now consider the second moment:

$$
\mu_{2} =
E[X^{2}] =
\int_{\alpha}^{\beta} \frac{1}{\beta-\alpha} x^2 dx =
$$

Solving the integral and setting $\mu_{2}$ equal to its theoretical value gives

$$
\mu_{2} =
\frac{\alpha^2 + \alpha\beta + \beta^2}{3} =
\frac{\sum_{i}^{5} X_{i}^2}{5} =
43.8
$$

$$
\mu_{2} =
131.4 =
\alpha^2 + \alpha\beta + \beta^2
$$

Since $\alpha + \beta$ = 11.6, solve for $\beta$ and plug into the equation for $\mu_2$:

$$
\beta = 11.6 - \alpha
$$

$$
131.4 = \alpha^2 + \alpha(11.6-\alpha) + (11.6-\alpha)^2
$$

$$
0 = 11.6\alpha + 134.6 - 23.2\alpha +\alpha^2 - 131.4
$$

$$
0 = \alpha^{2} - 11.6\alpha +4.2
$$

Using the quadratic formula, solving for $\alpha$ gives

$$
\alpha = .27 or 11.32
$$

Given the dataset, $\alpha$ = .27 makes more sense, since $X_{max}$ = 10. Now plug $\alpha$ in to solve for $\beta$:

$$
.27 + \beta = 11.6
\beta = 11.33
$$

Thus our method of moment estimates are $\hat{\alpha}_{MOM}$ = .27, $\hat{\beta}_{MOM}$ = 11.33.

## 6.20

### a

$$
L(\theta) =
\prod_{i=1}^{5} f(x_{i}|\theta) =
\prod_{i=1}^{5} \theta x_{i}^{\theta-1}
$$

$$
L(\theta) =
\theta(\prod_{i=1}^{5}x_{i})^{\theta - 1}
$$

Differentiate, set equal to 0, and solve for $\theta$:

$$
\frac{\partial L(\theta)}{\partial \theta} =
5\theta^{4} (\prod_{i = 1}^{5} x_{i})^{\theta-1} +
\theta^{5}(\prod_{i = 1}^{5} x_{i})^{\theta-1}ln(\prod_{i = 1}^{5} x_{i}) = 0
$$

$$
\theta^{4}(\prod_{i = 1}^{5} x_{i})^{\theta-1}(5 + \theta ln(\prod_{i = 1}^{5} x_{i})) = 0
$$

$$
\theta = \frac{-5}{ln(\prod_{i = 1}^{5} x_{i})} \approx 1.57
$$

Thus $\theta_{MLE} \approx$ 1.57.

### b

$$
\mu_{1} = E[X] = \int_{0}^{1}\theta x^{\theta-1} x dx = \int_{0}^{1}\theta x^{\theta} dx
$$

$$
\mu_{1} = \frac{\theta}{\theta+1}x^{\theta+1}|_{0}^{1} = \frac{\theta}{\theta+1}
$$

$$
\hat{\mu}_{1} = \frac{\theta}{\theta+1} = \bar{X}
$$

Rearranging gives:

$$
\theta = \frac{\bar{X}}{1-\bar{X}} = \frac{.594}{1-.594} = 1.463
$$

Thus $\theta_{MOM} \approx$ 1.463.

## 6.25

$\forall i \epsilon$ {1,..,n}, $a_{i} = \frac{1}{n}$

## 6.27

### a

bias = $E[\hat{\sigma}^{2}] - \sigma^{2}$

$$
E[\hat{\sigma}^{2}] =
E[\frac{1}{n} \sum_{i=1}^{n}(X_{i} - \bar{X})^2] = 
\frac{1}{n}E[\sum_{i=1}^{n}X_{i}^2 - n\bar{X}^2]
$$

$$
E[\hat{\sigma}^{2}] = \frac{1}{n}(\sum_{i=1}^{n}E[X_{i}^2] - E[n\bar{X}^2])
$$

$$
E[\hat{\sigma}^{2}] = \frac{n-1}{n}\sigma^{2}
$$

Thus bias = $\frac{n-1}{n}\sigma^{2} - \sigma^{2} = \frac{-\sigma^2}{n}$

### b 

From theorem B.16, $\frac{(n-1)S^2}{\sigma^2}$ ~ $\chi^2_{n-1}$, where $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_{i} - \bar{X})^2$. Also, from theorem B.12, V[$\frac{(n-1)S^2}{\sigma^2}$] = 2(n-1). Thus

$$
\frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{\sigma^2} \tilde{} \chi^2_{n-1}  
$$

Replace with $\hat{\sigma}^2$ to get

$$
\frac{\hat{\sigma}^2 n}{\sigma^2} \tilde{} \chi^2_{n-1}
$$

Thus

$$
V[\frac{\hat{\sigma}^2 n}{\sigma^2}] = 2(n-1)
$$

$$
V[\hat{\sigma}^2] = \frac{2(n-1)\sigma^{4}}{n^2}
$$

### c

$$
MSE[\hat{\sigma}^2] = bias[\hat{\sigma}^2]^{2} + V[\hat{\sigma}^2]
$$

$$
MSE[\hat{\sigma}^2] = bias[\hat{\sigma}^2]^{2} + V[\hat{\sigma}^2]
$$


$$
MSE[\hat{\sigma}^2] = \frac{\sigma^4}{n^2} + \frac{2(n-1)\sigma^{4}}{n^2}
$$

$$
MSE[\hat{\sigma}^2] = \frac{2n-\sigma^4}{n^2}
$$