---
title: "MATH 392 Problem Set 4"
author: "EJ Arce"
date: "17 February 2018"
output: 
  pdf_document:
    fig_height: 3
    fig_width: 6
---


```{r, include=FALSE}
require(tidyverse) # load packages
require(resampledata)
set.seed(30) # set seed for reproducibility
```

## 6.2

### Analytical Solution

Let $x_{1},...,x_{n}$ ~ Poisson($\lambda$). Show the maximum likelihood estimate of $\lambda$ is $\hat{\lambda}=\bar{x}$:

$$
L(\lambda|x_{1},...,x_{n}) = P(X_{1} = x_{1},...,X_{n}=x_{n})
$$

$$
L(\lambda|x_{1},...,x_{n}) = P(X = x_{1})...P(X=x_{n})
$$

$$
L(\lambda|x_{1},...,x_{n}) = \prod_{i}^{n}\frac{\lambda^{x_{i}}e^{-\lambda}}{x_{i}!}
$$

$$
L(\lambda|x_{1},...,x_{n}) = \frac{e^{-\lambda n}\lambda^{\sum_{i}^{n}{x_{i}}}}{\prod_{i}^{n}x_{i}!}
$$

Take the natural log of both sides to make the derivation easier:

$$
ln(L(\lambda|x_{1},...,x_{n})) =
ln(e^{-n\lambda}) +
ln(\lambda^{\sum_{i}^{n}{x_{i}}}) -
ln(\prod_{i}^{n}x_{i}!)
$$

$$
ln(L(\lambda|x_{1},...,x_{n})) =
-n\lambda +
ln(\lambda)\sum_{i}^{n}{x_{i}} -
ln(\prod_{i}^{n}x_{i}!)
$$

Derive with respect to $\lambda$ and set equal to 0:

$$
\frac{\partial ln(L(\lambda|x_{1},...,x_{n}))}{\partial \lambda} =
-n +
\frac{\sum_{i}^{n}{x_{i}}}{\lambda} =
0
$$

$$
\frac{\partial ln(L(\lambda|x_{1},...,x_{n}))}{\partial \lambda} =
\lambda = 
\frac{\sum_{i}^{n}{x_{i}}}{n} = \bar{x}
$$

Thus $\lambda_{MLE} = \bar{x}$

### Empirical Solution

```{r}
# Build function
e <- exp(1)
pois <- function(lambda, x){
  n <- length(x)
  sum.x <- sum(x)
  prod.fact <- prod(factorial(x))
  e^(-lambda*n)*(lambda^(sum.x))/(prod.fact)
}

# Create sequence of lambdas for MLE plot
lambdas <- seq(from = 0, to = 20, by = .05)
# Draw random sample of x's
x <- sample(1:10,30,replace=T)
# Apply pois function to find likelihoods
L_pois <- pois(lambdas,x)

# Plot
df <- data.frame(x = lambdas,
                 y = L_pois)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = mean(x), col = "red")
```

A red line above indicates the mean of the random sample. As expected, the highest likelihood for $\lambda$ is at the sample mean.

## 6.8

### Analytical Solution

$$
f(x;\theta) = \frac{\sqrt{2/\pi}x^{2}e^{-x^{2}/2\theta^{2}}}{\theta^{3}}
$$

$$
L(\theta|x_{1},...,x_{n}) =
\prod_{i}^{n}f(x_{i};\theta)
$$

$$
L(\theta|x_{1},...,x_{n}) =
\prod_{i}^{n}
\frac{\sqrt{2/\pi}x_{i}^{2}
e^{-x_{i}^{2}/2\theta^{2}}}
{\theta^{3}}
$$

$$
L(\theta|x_{1},...,x_{n}) =
(\frac{\sqrt{2/\pi}}
{\theta^3})^n
\prod_{i}^{n}
x_{i}^{2}
e^{-x_{i}^{2}/2\theta^{2}}
$$

$$
ln(L(\theta|x_{1},...,x_{n})) =
ln(\frac{\sqrt{2/\pi}}
{\theta^3})^n +
\sum_{i}^{n}
ln(x_{i}^{2}) +
\sum_{i}^{n}
ln(e^{-x_{i}^{2}/2\theta^{2}})
$$

$$
ln(L(\theta|x_{1},...,x_{n})) =
ln\sqrt{2/\pi} -
ln \theta^3 +
\sum_{i}^{n}
ln(x_{i}^{2}) +
\sum_{i}^{n}
-x_{i}^{2}/2\theta^{2}
$$

Derive with respect to $\theta$ and set to 0:

$$
\frac{\partial(L(\theta|x_{1},...,x_{n}))}{\partial\theta} =
-\frac{n}{\theta^3}3\theta^{2} + \sum_i^n{x_i^{2}/\theta^{3}} = 0
$$

$$
\frac{3n}{\theta} = \frac{\sum_i^n x_{i}^2}{\theta^3}
$$

$$
\theta = \frac{
\bar{x}
\sqrt{n}
}
{\sqrt{3}}
$$

Thus $\theta_{MLE}  = \frac{\bar{x}\sqrt{n}}{\sqrt{3}}$.

### Empirical Solution

```{r}
# Build function
f <- function(theta, x){
  e <- exp(1)
  n <- length(x)
  sec.term <- (x^2)*e^((-x^2)/(2*(theta)^2))
  prod.x <- prod(sec.term)
  ((sqrt(2/pi)/theta)^n)*prod.x
}

# Create sequence of thetas for MLE plot
thetas <- seq(from = .1, to = 30, by = .1)
# Reset seed and draw random sample of x's
set.seed(23)
x <- sample(1:10,30,replace=T)
# Apply function to find likelihoods
L_theta <- f(thetas,x)
# Find x-intercept from analytical solution
n <- length(x)
analytical.sol <- mean(x)*sqrt(n)/sqrt(3)
# Plot
df <- data.frame(x = thetas,
                 y = L_theta)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = analytical.sol, col = "red")
```

## 6.11

### Analytical Solution

As before,

$$
L(\lambda) = \prod_i^n f(x_{i}) \prod_j^m f(y_{j})
$$

$$
L(\lambda) =
\prod_i^n \lambda e^{-\lambda x_i}
\prod_j^m \lambda e^{-\lambda y_j}
$$

$$
L(\lambda) =
\lambda^{n+m}\lambda
e^{-\lambda(\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j})}
$$

$$
ln(L(\lambda)) =
(n+m)ln(\lambda) -
\lambda(\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j})
$$

$$
\frac
{\partial ln(L(\lambda))}
{\partial \lambda}=
(n+m)/\lambda -
(\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j}) = 0
$$

$$
\lambda =
\frac{n+m}
{\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j}}
$$

Thus, $\lambda_{MLE} = \frac{n+m}{\sum_{i}^{n}x_{i}+\sum_{j}^{m}y_{j}}$.

### Empirical Solution

```{r}
# Build function
e <- exp(1)
pois <- function(lambda,x,y){
  n <- length(x)
  m <- length(y)
  p.x <- (lambda^n)*prod((e^(-lambda*x)))
  p.y <- (lambda^m)*prod((e^(-lambda*y)))
  p.x*p.y
}

# Create sequence of lambdas for MLE plot
lambdas <- seq(from = 0, to = 20, by = .05)
# Draw random sample of x's
set.seed(11)
x <- sample(1:10,30,replace=T)
set.seed(35)
y <- sample(1:10,40,replace=T)
# Apply pois function to find likelihoods
L_pois <- pois(lambdas,x,y)
# Calculate analytical solution
obs <- (length(x)+length(y))/(sum(x) + sum(y))
# Plot
df <- data.frame(x = lambdas,
                 y = L_pois)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = obs, col = "red")
```


## 6.13

### a

$$
L(\alpha|X;\beta) =
\alpha \beta x^{\beta - 1} e^{-\alpha x^{\beta}}
$$

$$
ln(L(\alpha|X;\beta)) =
ln\alpha + ln\beta +(\beta - 1)lnx + ln(e^{-\alpha x^{\beta}})
$$

$$
ln(L(\alpha|X;\beta)) =
ln\alpha + ln\beta +(\beta - 1)lnx -\alpha x^{\beta}
$$

$$
\frac{\partial{ln(L(\alpha|X;\beta))}}{\partial \alpha}=
\frac{1}{\alpha} - x^{\beta} = 0
$$

$$
\alpha = \frac{1}{x^{\beta}}
$$

Thus $\alpha_{MLE} = \frac{1}{x^{\beta}}$.

### b

$$
L(\alpha;\beta|X) =
\alpha \beta x^{\beta - 1} e^{-\alpha x^{\beta}}
$$

Take the natural log of both sides:

$$
ln(L(\alpha;\beta|X)) =
ln \alpha + ln \beta + (\beta - 1)ln x -\alpha x^{\beta}
$$

Differentiate with respect to $\alpha$ and $\beta$ and set to 0. Solve simultaneously for a and b:

$$
\frac{\partial ln(L(\alpha|X))}
{\partial \alpha}=
1/\alpha - x^{\beta} = 0
$$

and

$$
\frac{\partial ln(L(\beta|X))}
{\partial \beta}=
1/\beta - +ln x - ax^{\beta} = 0
$$
