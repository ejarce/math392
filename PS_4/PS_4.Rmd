---
title: "MATH 392 Problem Set 4"
author: "EJ Arce"
date: "17 February 2018"
output: 
  pdf_document:
    fig_height: 3
    fig_width: 6
---


```{r, include=FALSE}
require(tidyverse) # load packages
require(resampledata)
set.seed(30) # set seed for reproducibility
```

## 6.2

### Analytical Solution

Let $x_{1},...,x_{n}$ ~ Poisson($\lambda$). Show the maximum likelihood estimate of $\lambda$ is $\hat{\lambda}=\bar{x}$:

$$
L(\lambda|x_{1},...,x_{n}) = P(X_{1} = x_{1},...,X_{n}=x_{n})
$$

$$
L(\lambda|x_{1},...,x_{n}) = P(X = x_{1})...P(X=x_{n})
$$

$$
L(\lambda|x_{1},...,x_{n}) = \prod_{i}^{n}\frac{\lambda^{x_{i}}e^{-\lambda}}{x_{i}!}
$$

$$
L(\lambda|x_{1},...,x_{n}) = \frac{e^{-\lambda n}\lambda^{\sum_{i}^{n}{x_{i}}}}{\prod_{i}^{n}x_{i}!}
$$

Take the natural log of both sides to make the derivation easier:

$$
ln(L(\lambda|x_{1},...,x_{n})) =
ln(e^{-n\lambda}) +
ln(\lambda^{\sum_{i}^{n}{x_{i}}}) -
ln(\prod_{i}^{n}x_{i}!)
$$

$$
ln(L(\lambda|x_{1},...,x_{n})) =
-n\lambda +
ln(\lambda)\sum_{i}^{n}{x_{i}} -
ln(\prod_{i}^{n}x_{i}!)
$$

Derive with respect to $\lambda$ and set equal to 0:

$$
\frac{\partial ln(L(\lambda|x_{1},...,x_{n}))}{\partial \lambda} =
-n +
\frac{\sum_{i}^{n}{x_{i}}}{\lambda} =
0
$$

$$
\frac{\partial ln(L(\lambda|x_{1},...,x_{n}))}{\partial \lambda} =
\lambda = 
\frac{\sum_{i}^{n}{x_{i}}}{n} = \bar{x}
$$

Thus $\lambda_{MLE} = \bar{x}$

### Empirical Solution

```{r}
# Build function
e <- exp(1)
pois <- function(lambda, x){
  n <- length(x)
  sum.x <- sum(x)
  prod.fact <- prod(factorial(x))
  e^(-lambda*n)*(lambda^(sum.x))/(prod.fact)
}

# Create sequence of lambdas for MLE plot
lambdas <- seq(from = 0, to = 20, by = .05)
# Draw random sample of x's
x <- sample(1:10,30,replace=T)
# Apply pois function to find likelihoods
L_pois <- pois(lambdas,x)

# Plot
df <- data.frame(x = lambdas,
                 y = L_pois)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = mean(x), col = "red")
```

A red line above indicates the mean of the random sample. As expected, the highest likelihood for $\lambda$ is at the sample mean.

## 6.8

### Analytical Solution

$$
f(x;\theta) = \frac{\sqrt{2/\pi}x^{2}e^{-x^{2}/2\theta^{2}}}{\theta^{3}}
$$

$$
L(\theta|x_{1},...,x_{n}) =
\prod_{i}^{n}f(x_{i};\theta)
$$

$$
L(\theta|x_{1},...,x_{n}) =
\prod_{i}^{n}
\frac{\sqrt{2/\pi}x_{i}^{2}
e^{-x_{i}^{2}/2\theta^{2}}}
{\theta^{3}}
$$

$$
L(\theta|x_{1},...,x_{n}) =
(\frac{\sqrt{2/\pi}}
{\theta^3})^n
\prod_{i}^{n}
x_{i}^{2}
e^{-x_{i}^{2}/2\theta^{2}}
$$

$$
ln(L(\theta|x_{1},...,x_{n})) =
ln((\frac{\sqrt{2/\pi}} {\theta^3})^n) +
\sum_{i}^{n}ln(x_{i}^{2}) +
\sum_{i}^{n}ln(e^{-x_{i}^{2}/2\theta^{2}})
$$

$$
ln(L(\theta|x_{1},...,x_{n})) =
n (ln\sqrt{2/\pi} - ln \theta^3) +
\sum_{i}^{n}ln(x_{i}^{2}) -
\frac{1}{2\theta^2}\sum_{i}^{n} x_{i}^{2}
$$

Derive with respect to $\theta$ and set to 0:

$$
\frac{\partial(lnL(\theta|x_{1},...,x_{n}))}{\partial\theta} =
-\frac{n}{\theta^3}3\theta^{2} + \frac{1}{\theta^3}\sum_i^n{x_i^{2}} = 0
$$

$$
\frac{3n}{\theta} = \frac{\sum_i^n x_{i}^2}{\theta^3}
$$

$$
\theta = \sqrt{
\frac{\sum_i^n x_{i}^2}{3n}
}
$$

Thus $\theta_{MLE}  = \frac{\bar{x}\sqrt{n}}{\sqrt{3}}$.

### Empirical Solution

```{r}
# Build log function
f <- function(theta, x){
  n <- length(x)
  n*(log(sqrt(2/pi)) - log(theta^3)) +
    sum(log(x^2)) -
    sum((x^2))/(2*(theta^2))
}

# Create sequence of thetas for MLE plot
thetas <- seq(from = .1, to = 60, by = .1)
# Reset seed and draw random sample of x's
set.seed(23)
x <- rnorm(30,5,2)
# Apply function to find likelihoods
L_theta <- f(thetas,x)
# Find x-intercept from analytical solution
n <- length(x)
analytical.sol <- sqrt(sum(x^2)/(3*n))
# Plot
df <- data.frame(x = thetas,
                 y = L_theta)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = analytical.sol, col = "red") +
  xlim(0,60) +
  ylim(-500,0)
```

As expected, the maximum likelihood estimate of our simulation is at about the analytical solution.

## 6.11

### Analytical Solution

$$
L(\lambda) = \prod_i^n f(x_{i}) \prod_j^m f(y_{j})
$$

$$
L(\lambda) =
\prod_i^n \lambda e^{-\lambda x_i}
\prod_j^m 2\lambda e^{-2\lambda y_j}
$$

$$
L(\lambda) =
2^m\lambda^{n+m}\lambda
e^{-\lambda(\sum_{i}^{n}x_{i}+2\sum_{j}^{m}y_{j})}
$$

$$
ln(L(\lambda)) =
(n+m)ln(\lambda) -
\lambda(\sum_{i}^{n}x_{i}+2\sum_{j}^{m}y_{j})
$$

$$
\frac
{\partial ln(L(\lambda))}
{\partial \lambda}=
(n+m)/\lambda -
(\sum_{i}^{n}x_{i}+2\sum_{j}^{m}y_{j}) = 0
$$

$$
\lambda =
\frac{n+m}
{\sum_{i}^{n}x_{i}+2\sum_{j}^{m}y_{j}}
$$

Thus, $\lambda_{MLE} = \frac{n+m}{\sum_{i}^{n}x_{i}+2\sum_{j}^{m}y_{j}}$.

### Empirical Solution

```{r}
# Build log function
pois <- function(lambda,x,y){
  n <- length(x)
  m <- length(y)
  (n+m)*log(lambda) - lambda*(sum(x) + 2*sum(y))
}

# Create sequence of lambdas for MLE plot
lambdas <- seq(from = 0, to = 20, by = .05)
# Draw random sample of x's
set.seed(11)
x <- sample(1:10,30,replace=T)
set.seed(35)
y <- sample(1:10,40,replace=T)
# Apply pois function to find likelihoods
L_pois <- pois(lambdas,x,y)
# Calculate analytical solution
obs <- (length(x)+length(y))/(sum(x) + 2*sum(y))
# Plot
df <- data.frame(x = lambdas,
                 y = L_pois)
ggplot(df, aes(x=x,y=y)) +
  geom_line() +
  geom_vline(xintercept = obs, col = "red") +
  xlim(0,1) +
  ylim(-500,0)
```

The analytical solution (the red line) is at around the maximum of the log likelihood function.

## 6.13

### a

$$
L(\alpha|X_1,...,X_n;\beta) =
\prod_i^n\alpha \beta X_{i}^{\beta - 1} e^{-\alpha X_{i}^{\beta}} =
(\alpha \beta)^n \prod_i^n X_{i}^{\beta - 1} e^{-\alpha X_{i}^{\beta}}
$$

$$
ln(L(\alpha)) =
nln\alpha +
nln\beta +
(\beta - 1)\sum_i^n ln X_{i} +
\sum_i^n ln(e^{-\alpha X_{i}^{\beta}})
$$

$$
ln(L(\alpha|X;\beta)) =
nln\alpha +
nln\beta +
(\beta - 1)\sum_i^n ln X_{i} -
\alpha \sum_i^n X_{i}^{\beta}
$$

$$
\frac{\partial{ln(L(\alpha|X;\beta))}}{\partial \alpha}=
\frac{n}{\alpha} - \sum_i^nX_{i}^{\beta} = 0
$$

Thus $\alpha_{MLE} = \frac{1}{n}\sum_i^nX_{i}^{\beta}$.

### b

$$
L(\alpha;\beta|X_1,...,X_n) =
(\alpha \beta)^n \prod_i^n X_{i}^{\beta - 1} e^{-\alpha X_{i}^{\beta}}
$$

Take the natural log of both sides:

$$
ln(L(\alpha;\beta|X)) =
nln\alpha +
nln\beta +
(\beta - 1)\sum_i^n ln X_{i} -
\alpha\sum_i^n X_{i}^{\beta}
$$

Differentiate with respect to $\alpha$ and $\beta$ and set to 0. Solve simultaneously for a and b:

$$
\frac{\partial ln(L(\alpha;\beta|X))}
{\partial \alpha}=
\frac{n}{\alpha} - \sum_i^nX_{i}^{\beta} =
0
$$

and

$$
\frac{\partial ln(L(alpha;\beta|X))}
{\partial \beta} =
\frac{n}{\beta} + ln(\sum_i^n X_{i}) - \alpha\sum_i^n X_{i}^{\beta} = 
0
$$

### Empirical Solution

```{r}
# Build log function
f <- function(alpha,beta,x){
  n <- length(x)
  n*log(alpha) +
    n*log(beta) +
    (beta-1)*sum(log(x)) -
    alpha*sum(x^beta)
}

# Create sequence of alphas and betas for plot
alphas <- seq(0,100,by=.1)
betas <- seq(0,100,by=.1)
# Draw random sample of x's
set.seed(11)
x <-rnorm(30,9,3)
# Apply function
L_ab <- f(alphas,betas,x)
df <- data.frame(alphas = alphas,
                 betas = betas,
                 L = L_ab)
ggplot(df, aes(alphas,betas)) +
  geom_tile(aes(fill = L))
```


## 6.14

Let X = 2,3,5,9,10 ~ Unif[$\alpha,\beta$]. Find $\hat{\alpha}_{MOM}$ and $\hat{\beta}_{MOM}$:

1. Write first and second moments in terms of parameters $\alpha$ and $\beta$

$$
\mu_{1} =
E[X] =
\int_{\alpha}^{\beta} \frac{1}{\beta-\alpha} x dx = 
\frac{1}{2(\beta-\alpha)} x^2 \vert_{\alpha}^{\beta} = 
\frac{\alpha +\beta}{2} = 
5.8 = 
\bar{X}
$$

Thus

$$
\alpha +\beta = 11.6
$$

Now consider the second moment:

$$
\mu_{2} =
E[X^{2}] =
\int_{\alpha}^{\beta} \frac{1}{\beta-\alpha} x^2 dx =
$$

Solving the integral and setting $\mu_{2}$ equal to its theoretical value gives

$$
\mu_{2} =
\frac{\alpha^2 + \alpha\beta + \beta^2}{3} =
\frac{\sum_{i}^{5} X_{i}^2}{5} =
43.8
$$

$$
\mu_{2} =
131.4 =
\alpha^2 + \alpha\beta + \beta^2
$$

Since $\alpha + \beta$ = 11.6, solve for $\beta$ and plug into the equation for $\mu_2$:

$$
\beta = 11.6 - \alpha
$$

$$
131.4 = \alpha^2 + \alpha(11.6-\alpha) + (11.6-\alpha)^2
$$

$$
0 = 11.6\alpha + 134.6 - 23.2\alpha +\alpha^2 - 131.4
$$

$$
0 = \alpha^{2} - 11.6\alpha +4.2
$$

Using the quadratic formula, solving for $\alpha$ gives

$$
\alpha = .27 or 11.32
$$

Given the dataset, $\alpha$ = .27 makes more sense, since $X_{max}$ = 10. Now plug $\alpha$ in to solve for $\beta$:

$$
.27 + \beta = 11.6
$$

$$
\beta = 11.33
$$

Thus our method of moment estimates are $\hat{\alpha}_{MOM}$ = .27, $\hat{\beta}_{MOM}$ = 11.33.

## 6.20

### a

$$
L(\theta) =
\prod_{i=1}^{5} f(x_{i}|\theta) =
\prod_{i=1}^{5} \theta x_{i}^{\theta-1}
$$

$$
L(\theta) =
\theta(\prod_{i=1}^{5}x_{i})^{\theta - 1}
$$

Differentiate, set equal to 0, and solve for $\theta$:

$$
\frac{\partial L(\theta)}{\partial \theta} =
5\theta^{4} (\prod_{i = 1}^{5} x_{i})^{\theta-1} +
\theta^{5}(\prod_{i = 1}^{5} x_{i})^{\theta-1}ln(\prod_{i = 1}^{5} x_{i}) = 0
$$

$$
\theta^{4}(\prod_{i = 1}^{5} x_{i})^{\theta-1}(5 + \theta ln(\prod_{i = 1}^{5} x_{i})) = 0
$$

$$
\theta = \frac{-5}{ln(\prod_{i = 1}^{5} x_{i})} \approx 1.57
$$

Thus $\theta_{MLE} \approx$ 1.57.

### b

$$
\mu_{1} = E[X] = \int_{0}^{1}\theta x^{\theta-1} x dx = \int_{0}^{1}\theta x^{\theta} dx
$$

$$
\mu_{1} = \frac{\theta}{\theta+1}x^{\theta+1}|_{0}^{1} = \frac{\theta}{\theta+1}
$$

$$
\hat{\mu}_{1} = \frac{\theta}{\theta+1} = \bar{X}
$$

Rearranging gives:

$$
\theta = \frac{\bar{X}}{1-\bar{X}} = \frac{.594}{1-.594} = 1.463
$$

Thus $\theta_{MOM} \approx$ 1.463.

## 6.25

$\forall i \epsilon$ {1,..,n}, $\sum_i^n a_{i} = n$

## 6.27

### a

bias = $E[\hat{\sigma}^{2}] - \sigma^{2}$

$$
E[\hat{\sigma}^{2}] =
E[\frac{1}{n} \sum_{i=1}^{n}(X_{i} - \bar{X})^2] = 
\frac{1}{n}E[\sum_{i=1}^{n}X_{i}^2 - n\bar{X}^2]
$$

$$
E[\hat{\sigma}^{2}] = \frac{1}{n}(\sum_{i=1}^{n}E[X_{i}^2] - E[n\bar{X}^2])
$$

$$
E[\hat{\sigma}^{2}] = \frac{n-1}{n}\sigma^{2}
$$

Thus bias = $\frac{n-1}{n}\sigma^{2} - \sigma^{2} = \frac{-\sigma^2}{n}$

### b 

From theorem B.16, $\frac{(n-1)S^2}{\sigma^2}$ ~ $\chi^2_{n-1}$, where $S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_{i} - \bar{X})^2$. Also, from theorem B.12, V[$\frac{(n-1)S^2}{\sigma^2}$] = 2(n-1). Thus

$$
\frac{\sum_{i=1}^n (X_{i} - \bar{X})^2}{\sigma^2} \tilde{} \chi^2_{n-1}  
$$

Replace with $\hat{\sigma}^2$ to get

$$
\frac{\hat{\sigma}^2 n}{\sigma^2} \tilde{} \chi^2_{n-1}
$$

Thus

$$
V[\frac{\hat{\sigma}^2 n}{\sigma^2}] = 2(n-1)
$$

$$
V[\hat{\sigma}^2] = \frac{2(n-1)\sigma^{4}}{n^2}
$$

### c

$$
MSE[\hat{\sigma}^2] = bias[\hat{\sigma}^2]^{2} + V[\hat{\sigma}^2]
$$

$$
MSE[\hat{\sigma}^2] = bias[\hat{\sigma}^2]^{2} + V[\hat{\sigma}^2]
$$


$$
MSE[\hat{\sigma}^2] = \frac{\sigma^4}{n^2} + \frac{2(n-1)\sigma^{4}}{n^2}
$$

$$
MSE[\hat{\sigma}^2] = \frac{2n-\sigma^4}{n^2}
$$

## 6.30

### a

$$
MSE_{\hat{\theta}_1} = bias_{\hat{\theta}_1}^2 + Var[{\hat{\theta}_1}] = 0 + 25 = 25
$$

$$
MSE_{\hat{\theta}_2} = bias_{\hat{\theta}_2}^2 + Var[{\hat{\theta}_2}] = 3^2 + 4 = 13
$$

Thus $MSE_{\hat{\theta}_2} \leq MSE_{\hat{\theta}_1}$.

### b

$$
MSE_{\hat{\theta}_2} \leq MSE_{\hat{\theta}_1}
$$

$$
b^2 + 4 \leq 25
$$

$$
b \leq 4.583
$$

## 6.31

```{r}
# Recreate plots using code from sec 6.3.3, changing the sample size
n <- 30
curve(x*(1-x)/n,from=0,to=1,xlab="p",ylab="MSE")
curve(n*(1-x)*x/(n+2)^2+(1-2*x)^2/(n+2)^2, add=TRUE, col="blue",lty=1)
n <- 50
curve(x*(1-x)/n,from=0,to=1,xlab="p",ylab="MSE")
curve(n*(1-x)*x/(n+2)^2+(1-2*x)^2/(n+2)^2, add=TRUE, col="blue",lty=1)
n <- 100
curve(x*(1-x)/n,from=0,to=1,xlab="p",ylab="MSE")
curve(n*(1-x)*x/(n+2)^2+(1-2*x)^2/(n+2)^2, add=TRUE, col="blue",lty=1)
n <- 200
curve(x*(1-x)/n,from=0,to=1,xlab="p",ylab="MSE")
curve(n*(1-x)*x/(n+2)^2+(1-2*x)^2/(n+2)^2, add=TRUE, col="blue",lty=1)
```

As n increases, the MSE decreases. In each case, the second estimator (the blue line) has a lower MSE except for low and high estimates of p.

## 6.37

### a

$$
E[\bar{X}] = E[\frac{X_1 + X_2}{2}] = E[X_1]/2 +E[X_2]/2 = E[X]/2 +E[X]/2 = E[X] = 1/\lambda
$$

### b

$$
Var[\bar{X}] =
Var[\frac{X_1 + X_2}{2}] =
\frac{1}{4}Var[X_1 + X_2] =
\frac{1}{4}(Var[X] + Var[X]) = 
\frac{1}{4}(\frac{1}{\lambda^2} + \frac{1}{\lambda^2}) =
\frac{2}{\lambda^2}
$$

### c

$$
E[\sqrt{X_1X_2}] = E[\sqrt{X_1}\sqrt{X_2}] = E[\sqrt{X_1}]E[\sqrt{X_2}] 
$$

By fact,

$$
E[\sqrt{X_1X_2}] = \sqrt{\pi}/2\sqrt{\lambda} (\sqrt{\pi}/2\sqrt{\lambda})) = \pi/4\lambda
$$

### d

$$
bias = |E[\sqrt{X_1X_2}] - 1/\lambda| = |\pi/4\lambda - 1/\lambda| = |\frac{\pi - 4}{4\lambda}|
$$

Thus bias = $0.2146/\lambda$.

